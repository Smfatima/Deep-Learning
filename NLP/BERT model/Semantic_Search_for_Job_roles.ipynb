{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "We are building a Semantic search engine to find relavant job roles uing the job describtion or prompt"
      ],
      "metadata": {
        "id": "Niy88j-l1q5T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ENDPOINT=\"https://localhost:9200/\"\n",
        "USERNAME=\"XXX\"\n",
        "PASSWORD=\"XXX\""
      ],
      "metadata": {
        "id": "TwZW5hhFqGlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U sentence-transformers\n"
      ],
      "metadata": {
        "id": "UuQ_DUFH13bi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install transformers\n"
      ],
      "metadata": {
        "id": "LyALEsm314oO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries \n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "import uuid"
      ],
      "metadata": {
        "id": "sHbhYFw72BrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install elasticsearch"
      ],
      "metadata": {
        "id": "mAY89csQ2XqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "import elasticsearch\n",
        "from elasticsearch import Elasticsearch\n",
        "from elasticsearch import helpers\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "tqdm.pandas()"
      ],
      "metadata": {
        "id": "CxlnEZmY2JLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filepath = \"data job posts.csv\"\n",
        "\n",
        "df  = pd.read_csv(filepath, engine='python', encoding='utf-8', error_bad_lines=False)\n",
        "\n"
      ],
      "metadata": {
        "id": "xMo2lzfk2Qfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "BsInhnYq2qKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is Sample class that will help to convert sentences into Token"
      ],
      "metadata": {
        "id": "WJfWEPdxO0No"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Tokenizer(object):\n",
        "    def __init__(self):\n",
        "        self.model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "    def get_token(self, documents):\n",
        "        sentences  = [documents]\n",
        "        sentence_embeddings = self.model.encode(sentences)\n",
        "        encod_np_array = np.array(sentence_embeddings)\n",
        "        encod_list = encod_np_array.tolist()\n",
        "        return encod_list[0]"
      ],
      "metadata": {
        "id": "zOnIw0sY58e2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Creating Tokens\n",
        "Lets create corups of vectors and use teh power of elastic search to Search vectors with semantics Search"
      ],
      "metadata": {
        "id": "1yNHU620YWHx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token_instance = Tokenizer()\n"
      ],
      "metadata": {
        "id": "2D1D9glgO8vv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.head(5000)\n",
        "df = df.dropna(how='all')\n",
        "df['vector'] = df['jobpost'].progress_apply(token_instance.get_token)"
      ],
      "metadata": {
        "id": "ItRudxVCYb-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "elk_data = df.to_dict(\"records\")"
      ],
      "metadata": {
        "id": "ruP_MYoqrs3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install elasticsearch==7.17"
      ],
      "metadata": {
        "id": "kmjLloR6yJp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "es = Elasticsearch(hosts=[ENDPOINT],  http_auth=(USERNAME, PASSWORD))\n",
        "es.ping()"
      ],
      "metadata": {
        "id": "GiLchEUHx4yi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x in elk_data:\n",
        "    try:\n",
        "        _={\n",
        "           \"title\":x.get(\"Title\", \"\"),\n",
        "            \"company\":x.get(\"Company\", \"\"),\n",
        "            \"location\":x.get(\"Location\", \"\"),\n",
        "            \"salary\":x.get(\"Salary\", \"\"),\n",
        "            \"vector\":x.get(\"vector\", \"\"),\n",
        "            \"job_description\":x.get(\"JobDescription\", \"\"),\n",
        "\n",
        "        }\n",
        "        es.index(index='posting1',body=_)\n",
        "    except Exception as e:pass"
      ],
      "metadata": {
        "id": "ckGGwDolsJdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test For Semantics Search\n"
      ],
      "metadata": {
        "id": "jI7NY5O9xlkX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT = input(\"Enter the Input Query \")\n",
        "token_vector = token_instance.get_token(INPUT)\n",
        "\n",
        "query  ={\n",
        "  \"_source\": [\"title\", \"job_description\"], \n",
        "   \"size\":50,\n",
        "   \"query\":{\n",
        "      \"bool\":{\n",
        "         \"must\":[\n",
        "            {\n",
        "               \"knn\":{\n",
        "                  \"vector\":{\n",
        "                     \"vector\":token_vector,\n",
        "                     \"k\":20\n",
        "                  }\n",
        "               }\n",
        "            }\n",
        "         ]\n",
        "      }\n",
        "   }\n",
        "}\n",
        "\n",
        "\n",
        "res = es.search(index='posting1',\n",
        "                size=50,\n",
        "                body=query,\n",
        "                request_timeout=55)\n",
        "\n",
        "title = [x['_source']  for x in res['hits']['hits']] \n",
        "title"
      ],
      "metadata": {
        "id": "4Wb4w0oXxddI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1RTzllIVxxVH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}